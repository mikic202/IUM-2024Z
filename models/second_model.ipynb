{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from random import sample\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import math\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'embeding_models/embeding_v1_16_dim.pt'\n",
    "model = torch.jit.load(model_path).encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1735858800.0\n"
     ]
    }
   ],
   "source": [
    "BASE_DATE = datetime.strptime(\"2025-01-03\", '%Y-%m-%d').timestamp()\n",
    "print(BASE_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_raw_data = pd.read_json(Path('../data_v2/tracks_artists.jsonl'), lines=True)\n",
    "\n",
    "\n",
    "# NORMALIZE DATES\n",
    "tracks_raw_data[\"release_date\"] = (pd.to_datetime(tracks_raw_data[\"release_date\"], format='mixed').apply(lambda x: x.timestamp())).div(BASE_DATE)\n",
    "\n",
    "# NORMALIZE DURATION\n",
    "tracks_raw_data[\"duration_ms\"] = tracks_raw_data[\"duration_ms\"].div(tracks_raw_data[\"duration_ms\"].max())\n",
    "\n",
    "# NORMALIZE TEMPO\n",
    "tracks_raw_data[\"tempo\"] = tracks_raw_data[\"tempo\"].div(tracks_raw_data[\"tempo\"].max())\n",
    "\n",
    "# EXPLICITE ENCODING\n",
    "tracks_raw_data[\"explicit\"] = tracks_raw_data[\"explicit\"].apply(lambda x: [0, 1] if x else [1, 0])\n",
    "\n",
    "# processs ARTIST HASH\n",
    "\n",
    "def postprocess_hash_to_list(x):\n",
    "    str_x = str(x)\n",
    "    if len(str_x) < 8:\n",
    "        str_x = \"0\" * (8 - len(str_x)) + str_x\n",
    "    return [int(x) for x in str_x]\n",
    "\n",
    "tracks_raw_data[\"id_artist_hash\"] = tracks_raw_data[\"id_artist_hash\"].apply(postprocess_hash_to_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracksDataset(Dataset):\n",
    "    def __init__(self, tracks_data: pd.DataFrame):\n",
    "        self.data = tracks_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_item(self, idx):\n",
    "        return self.data.iloc[idx].values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        unpacked_data = []\n",
    "        for data in self.data.iloc[idx].drop(\"id_track\").values:\n",
    "            if type(data) != list:\n",
    "                unpacked_data.append(data)\n",
    "            else:\n",
    "                unpacked_data += data\n",
    "        return torch.Tensor(unpacked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings_per_id = pd.DataFrame()\n",
    "embedings_per_id[\"id_track\"] = tracks_raw_data[\"id_track\"]\n",
    "embedings_per_id[\"embeding\"] = [model(torch.Tensor(x)).detach().cpu().numpy() for x in TracksDataset(tracks_raw_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_intigers = [687, 852, 528, 562, 426, 1092, 171, 250, 223, 265, 981, 607, 738, 1020, 510, 899, 596, 1047, 826, 669, 923, 905, 1063, 139, 1031]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id               timestamp  user_id                track_id  \\\n",
      "0       89426 2024-09-17 10:51:23.000      260  1xPec5BN0Zxv77zrWKq43S   \n",
      "1       89427 2023-05-20 00:35:36.227      260  7s0lDK7y3XLmI7tcsRAbW0   \n",
      "2       89427 2023-05-20 00:38:48.054      260  48lQegoLqGAzaRLnMwK0mO   \n",
      "3       89427 2023-05-20 00:43:30.802      260  48lQegoLqGAzaRLnMwK0mO   \n",
      "4       89427 2023-05-20 00:44:59.387      260  4usVYcPlxRgRet6YashdCJ   \n",
      "\n",
      "  event_type  \n",
      "0       play  \n",
      "1       play  \n",
      "2       play  \n",
      "3       like  \n",
      "4       play  \n"
     ]
    }
   ],
   "source": [
    "files_to_load = [file for file in os.listdir(\"../data_v2/sessions\") if int(file.split(\".\")[0].split(\"_\")[-1]) not in test_intigers]\n",
    "\n",
    "raw_sessions_data = pd.concat([pd.read_json(Path(f'../data_v2/sessions/{file}'), lines=True) for file in files_to_load])\n",
    "print(raw_sessions_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sesions_data = raw_sessions_data.merge(embedings_per_id, left_on=\"track_id\", right_on=\"id_track\").drop(\"id_track\", axis=1)\n",
    "merged_sesions_data[\"timestamp\"] = (pd.to_datetime(merged_sesions_data[\"timestamp\"], format='mixed').apply(lambda x: x.timestamp())).div(BASE_DATE)\n",
    "merged_sesions_data = pd.get_dummies(merged_sesions_data, columns=[\"event_type\"], dtype = int)\n",
    "\n",
    "merged_sesions_data = [pd.DataFrame(y) for _, y in merged_sesions_data.groupby('session_id', as_index=False) if len(y) > 1]\n",
    "\n",
    "NUMBER_OF_INPUTS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUserSesionsDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_item(self, idx):\n",
    "        return self.data.iloc[idx].values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sessions = []\n",
    "        for session in self.data[idx].drop(\"track_id\", axis=1).drop(\"session_id\", axis=1).drop(\"user_id\", axis=1).values:\n",
    "            unpacked_data = np.array([])\n",
    "            for data in session:\n",
    "                unpacked_data = np.append(unpacked_data, data)\n",
    "            sessions.append(torch.tensor(unpacked_data))\n",
    "        return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sesions_dataset = CustomUserSesionsDataset(merged_sesions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARE_PERCENTAGE = 0.25\n",
    "\n",
    "def pad_sessions_collate(batch, pad_value=0.0):\n",
    "    input_sequences = [\n",
    "    ]\n",
    "    output_sequences = [\n",
    "    ]\n",
    "\n",
    "    for batch_element in batch:\n",
    "        number_of_sequence_data = len(batch_element)\n",
    "        number_of_reference_data = math.ceil(number_of_sequence_data * COMPARE_PERCENTAGE)\n",
    "\n",
    "        input_sequences.append(torch.stack(batch_element[:number_of_sequence_data - number_of_reference_data]))\n",
    "        output_sequences.append(sample(batch_element[number_of_sequence_data - number_of_reference_data:], 1)[0])\n",
    "\n",
    "    x_lens = [len(x) for x in input_sequences]\n",
    "\n",
    "    padded_input_sequence = pad_sequence(input_sequences, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "    return padded_input_sequence, torch.stack([x[1:17] for x in output_sequences]), x_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(user_sesions_dataset, batch_size=8192, shuffle=True, collate_fn=pad_sessions_collate, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsertPreferenceGenerator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UsertPreferenceGenerator, self).__init__()\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 128\n",
    "        self.lstm = torch.nn.LSTM(input_size=NUMBER_OF_INPUTS, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(self.hidden_size, 16)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        state = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "\n",
    "    def forward(self, x, x_lens, hidden):\n",
    "\n",
    "        x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False).to(device)\n",
    "        packed_output, hidden = self.lstm(x_packed, hidden)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        x = self.linear(self.activation(output))\n",
    "\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UsertPreferenceGenerator(\n",
      "  (lstm): LSTM(20, 128, batch_first=True)\n",
      "  (activation): ReLU()\n",
      "  (linear): Linear(in_features=128, out_features=16, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikic202/miniconda3/envs/pt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = UsertPreferenceGenerator().to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss_fun = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UsertPreferenceGenerator(\n",
      "  (lstm): LSTM(20, 128, batch_first=True)\n",
      "  (activation): ReLU()\n",
      "  (linear): Linear(in_features=128, out_features=16, bias=True)\n",
      ")\n",
      "Epoch: 0, loss: 2.27e+02\n",
      "Epoch: 1, loss: 2.25e+02\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model = UsertPreferenceGenerator().to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss_fun = nn.L1Loss()\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(6):\n",
    "    loss_sum = 0\n",
    "    for x, targets, x_lens in train_loader:\n",
    "        x = x.float().to(device)\n",
    "        targets = targets.to(torch.float).to(device)\n",
    "\n",
    "        hidden, state = model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "\n",
    "        preds, _ = model(x, x_lens, (hidden, state))\n",
    "        preds = preds.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        last_pred = preds[:, -1, :]\n",
    "        loss = loss_fun(last_pred, targets)\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, loss: {loss_sum:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             aten::_pad_packed_sequence        10.52%       1.156ms        32.01%       3.519ms       3.519ms     909.000us         8.04%       3.522ms       3.522ms             1  \n",
      "                                             aten::sort        12.44%       1.368ms        27.00%       2.968ms       1.484ms       1.377ms        12.18%       2.978ms       1.489ms             2  \n",
      "                            aten::_pack_padded_sequence         6.33%     696.000us        24.89%       2.736ms       2.736ms     561.000us         4.96%       2.739ms       2.739ms             1  \n",
      "                                            aten::slice        11.40%       1.253ms        14.70%       1.616ms      11.145us       1.564ms        13.84%       2.080ms      14.345us           145  \n",
      "                                       aten::contiguous         1.33%     146.000us        11.77%       1.294ms      46.214us     193.000us         1.71%       1.370ms      48.929us            28  \n",
      "                                             aten::lstm         0.66%      73.000us        12.34%       1.356ms       1.356ms      75.000us         0.66%       1.359ms       1.359ms             1  \n",
      "                                            aten::copy_         5.47%     601.000us        11.93%       1.311ms      21.492us       1.299ms        11.49%       1.299ms      21.295us            61  \n",
      "                                       aten::_cudnn_rnn        10.02%       1.101ms        11.42%       1.255ms       1.255ms       1.210ms        10.71%       1.262ms       1.262ms             1  \n",
      "                                            aten::clone         3.03%     333.000us         9.93%       1.092ms      39.000us     341.000us         3.02%       1.177ms      42.036us            28  \n",
      "                                     aten::index_select         2.27%     249.000us         3.69%     406.000us      67.667us     814.000us         7.20%     929.000us     154.833us             6  \n",
      "                                           aten::arange         3.99%     439.000us         8.31%     914.000us     228.500us     453.000us         4.01%     927.000us     231.750us             4  \n",
      "                                       aten::as_strided         0.65%      71.000us         0.65%      71.000us       0.464us     549.000us         4.86%     549.000us       3.588us           153  \n",
      "                                           aten::linear         0.27%      30.000us         1.49%     164.000us     164.000us      21.000us         0.19%     478.000us     478.000us             1  \n",
      "                                       aten::empty_like         1.46%     160.000us         3.24%     356.000us      12.276us     215.000us         1.90%     437.000us      15.069us            29  \n",
      "                                            aten::addmm         0.66%      73.000us         0.80%      88.000us      88.000us     416.000us         3.68%     416.000us     416.000us             1  \n",
      "                                            aten::empty         2.38%     262.000us         2.38%     262.000us       5.574us     391.000us         3.46%     391.000us       8.319us            47  \n",
      "                                             aten::view         1.02%     112.000us         1.02%     112.000us       1.836us     327.000us         2.89%     327.000us       5.361us            61  \n",
      "                                             aten::full         0.31%      34.000us         1.08%     119.000us     119.000us      29.000us         0.26%     183.000us     183.000us             1  \n",
      "                                               aten::to         0.20%      22.000us         4.27%     469.000us     117.250us      28.000us         0.25%     163.000us      40.750us             4  \n",
      "                                         aten::_to_copy         0.34%      37.000us         4.03%     443.000us     221.500us      31.000us         0.27%     135.000us      67.500us             2  \n",
      "                                            aten::fill_         0.43%      47.000us         0.57%      63.000us      63.000us     133.000us         1.18%     133.000us     133.000us             1  \n",
      "                                            aten::index         0.64%      70.000us         0.75%      82.000us      82.000us      73.000us         0.65%      87.000us      87.000us             1  \n",
      "                                          aten::resize_         0.46%      51.000us         0.46%      51.000us       6.375us      81.000us         0.72%      81.000us      10.125us             8  \n",
      "                                         aten::scatter_         0.33%      36.000us         0.44%      48.000us      48.000us      45.000us         0.40%      53.000us      53.000us             1  \n",
      "                                              aten::cat         0.25%      27.000us         0.30%      33.000us      33.000us      52.000us         0.46%      52.000us      52.000us             1  \n",
      "                                        aten::transpose         0.20%      22.000us         0.29%      32.000us      10.667us      28.000us         0.25%      40.000us      13.333us             3  \n",
      "                                          aten::reshape         0.10%      11.000us         0.18%      20.000us      10.000us      16.000us         0.14%      27.000us      13.500us             2  \n",
      "                                                aten::t         0.09%      10.000us         0.19%      21.000us      21.000us      12.000us         0.11%      24.000us      24.000us             1  \n",
      "                                    aten::empty_strided         0.21%      23.000us         0.21%      23.000us      11.500us      21.000us         0.19%      21.000us      10.500us             2  \n",
      "                                           aten::select         0.12%      13.000us         0.15%      16.000us      16.000us      15.000us         0.13%      19.000us      19.000us             1  \n",
      "                                             aten::set_         0.08%       9.000us         0.12%      13.000us       6.500us      14.000us         0.12%      19.000us       9.500us             2  \n",
      "                                       aten::lift_fresh         0.01%       1.000us         0.01%       1.000us       1.000us       5.000us         0.04%       5.000us       5.000us             1  \n",
      "                              aten::cudnn_is_acceptable         0.01%       1.000us         0.01%       1.000us       1.000us       3.000us         0.03%       3.000us       3.000us             1  \n",
      "                                        cudaEventRecord        12.13%       1.334ms        12.13%       1.334ms       1.091us       0.000us         0.00%       0.000us       0.000us          1223  \n",
      "                                        cudaMemcpyAsync         5.19%     570.000us         5.19%     570.000us      19.655us       0.000us         0.00%       0.000us       0.000us            29  \n",
      "                                  cudaStreamSynchronize         0.13%      14.000us         0.13%      14.000us       7.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                       cudaLaunchKernel         3.03%     333.000us         3.03%     333.000us       3.202us       0.000us         0.00%       0.000us       0.000us           104  \n",
      "                                  cudaStreamIsCapturing         0.02%       2.000us         0.02%       2.000us       1.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                  cudaStreamGetPriority         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                       cudaDeviceGetStreamPriorityRange         0.01%       1.000us         0.01%       1.000us       0.500us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                    cudaStreamWaitEvent         0.03%       3.000us         0.03%       3.000us       0.120us       0.000us         0.00%       0.000us       0.000us            25  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.05%       5.000us         0.05%       5.000us       0.333us       0.000us         0.00%       0.000us       0.000us            15  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.04%       4.000us         0.04%       4.000us       4.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize         1.73%     190.000us         1.73%     190.000us     190.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 10.993ms\n",
      "Self CUDA time total: 11.301ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-12-24 14:20:35 4443:4443 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-12-24 14:20:35 4443:4443 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-12-24 14:20:35 4443:4443 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "with profiler.profile(use_cuda=True) as prof:\n",
    "    preds, _ = model(x, x_lens, (hidden, state))\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.993923\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4.7692013\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m0.50016433\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m1.8391552\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4.93348\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m1.0898505\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m1.9289817\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10.784728\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3.9150748\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m5.587146\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m7.239292\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m7.1324625\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m2.55823\u001b[39;49m\u001b[43m   \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m4.176082\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m8.504707\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m5.0005746\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2.202329\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mUsertPreferenceGenerator.forward\u001b[0;34m(self, x, x_lens, hidden)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_lens, hidden):\n\u001b[0;32m---> 16\u001b[0m     x_packed \u001b[38;5;241m=\u001b[39m \u001b[43mpack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     packed_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x_packed, hidden)\n\u001b[1;32m     18\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m pad_packed_sequence(packed_output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/nn/utils/rnn.py:259\u001b[0m, in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     lengths, sorted_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(lengths, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 259\u001b[0m     sorted_indices \u001b[38;5;241m=\u001b[39m sorted_indices\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m    260\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mindex_select(batch_dim, sorted_indices)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "model([[0.993923, 4.7692013 ,  0.50016433,  1.8391552 , -4.93348   ,  1.0898505 ,\n",
    "          1.9289817 , 10.784728  , -3.9150748 ,  5.587146  ,  7.239292  ,\n",
    "          7.1324625 ,  2.55823   ,  4.176082  ,  8.504707  ,  5.0005746 ,\n",
    "         -2.202329, 0, 1, 0]], [1], torch.zeros(1, 1, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "UsertPreferenceGenerator.forward() missing 2 required positional arguments: 'x_lens' and 'hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_sesions_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: UsertPreferenceGenerator.forward() missing 2 required positional arguments: 'x_lens' and 'hidden'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
